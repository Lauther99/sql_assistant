{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciando Componentes\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"C:\\\\Users\\\\lauth\\\\OneDrive\\\\Desktop\\\\sql_assistant_v3\")\n",
    "from src.components.memory.memory import Memory\n",
    "from src.components.collector.collector import AppDataCollector, LLMResponseCollector\n",
    "from src.components.models.llms.llms import Langchain_OpenAI_LLM, HF_Llama38b_LLM\n",
    "from src.components.models.embeddings.embeddings import HF_MultilingualE5_Embeddings, Openai_Embeddings\n",
    "from src.components.memory.memory import Memory\n",
    "from src.components.memory.memory_interfaces import AIMessage, HumanMessage\n",
    "from typing import List, Union\n",
    "\n",
    "# Iniciando modelos LLM\n",
    "langchain_llm = Langchain_OpenAI_LLM()\n",
    "langchain_llm.init_model()\n",
    "hf_llm = HF_Llama38b_LLM()\n",
    "hf_llm.init_model()\n",
    "\n",
    "# Iniciando modelos Embeddings\n",
    "mle5_embeddings = HF_MultilingualE5_Embeddings()\n",
    "mle5_embeddings.init_model()\n",
    "\n",
    "openai_embeddings = Openai_Embeddings()\n",
    "openai_embeddings.init_model()\n",
    "\n",
    "# Iniciando memoria\n",
    "messages: List[Union[AIMessage, HumanMessage]]  = []\n",
    "memory = Memory(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciando workflow\n",
    "\n",
    "from src.app.pipeline_processes.query_post_process.manager import query_post_process\n",
    "from src.app.pipeline_processes.query_pre_process.manager import (\n",
    "    query_pre_process,\n",
    "    simple_request_process,\n",
    ")\n",
    "from src.app.pipeline_processes.sql_generation_process.manager import (\n",
    "    complex_request_sql_generation,\n",
    ")\n",
    "from src.app.pipeline_processes.sql_post_process.manager import (\n",
    "    complex_request_pre_query_generation,\n",
    "    complex_request_sql_summary_response,\n",
    "    complex_request_sql_verification,\n",
    ")\n",
    "from src.app.pipeline_processes.sql_pre_process.manager import (\n",
    "    complex_request_process_modification,\n",
    "    complex_request_process_semantics,\n",
    ")\n",
    "from src.utils.sql_utils import run_sql\n",
    "import traceback\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def pre_process_pipeline(\n",
    "    memory: Memory,\n",
    "    collector: AppDataCollector,\n",
    "    llm_collector: LLMResponseCollector,\n",
    "):\n",
    "    try:\n",
    "        query_pre_process(\n",
    "            llm=hf_llm,\n",
    "            memory=memory,\n",
    "            collector=collector,\n",
    "            llm_collector=llm_collector,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error en: query_pre_process:\\n {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "def simple_request_pipeline(\n",
    "    collector: AppDataCollector, llm_collector: LLMResponseCollector\n",
    "):\n",
    "    try:\n",
    "        simple_request_process(\n",
    "            llm=hf_llm,\n",
    "            collector=collector,\n",
    "            llm_collector=llm_collector,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error en: simple_request_process:\\n {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "def complex_request_pipeline(\n",
    "    collector: AppDataCollector, llm_collector: LLMResponseCollector, memory: Memory\n",
    "):\n",
    "    try:\n",
    "        print(\"complex_request_process_modification\")\n",
    "        complex_request_process_modification(\n",
    "            llm=hf_llm,\n",
    "            embeddings=mle5_embeddings,\n",
    "            memory=memory,\n",
    "            collector=collector,\n",
    "            llm_collector=llm_collector,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error en: complex_request_process_modification:\\n {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    try:\n",
    "        print(\"complex_request_process_semantics\")\n",
    "        complex_request_process_semantics(\n",
    "            llm=hf_llm,\n",
    "            embeddings=openai_embeddings,\n",
    "            collector=collector,\n",
    "            llm_collector=llm_collector,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error en: complex_request_process_semantics:\\n {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    try:\n",
    "        print(\"complex_request_sql_generation\")\n",
    "        complex_request_sql_generation(\n",
    "            llm=hf_llm, collector=collector, llm_collector=llm_collector\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error en: complex_request_sql_generation:\\n {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    try:\n",
    "        print(\"complex_request_sql_verification\")\n",
    "        complex_request_sql_verification(\n",
    "            llm=hf_llm, collector=collector, llm_collector=llm_collector\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error en: complex_request_sql_verification:\\n {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    is_prequery = collector.assistant_sql_code_class.strip() == \"incomplete\"\n",
    "    if is_prequery:\n",
    "        try:\n",
    "            print(\"complex_request_pre_query_generation\")\n",
    "            complex_request_pre_query_generation(\n",
    "                llm=langchain_llm, collector=collector, llm_collector=llm_collector\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error en: complex_request_pre_query_generation:\\n {e}\")\n",
    "            traceback.print_exc()\n",
    "            \n",
    "       \n",
    "def post_sql_generation_pipeline(\n",
    "    collector: AppDataCollector, llm_collector: LLMResponseCollector\n",
    "):\n",
    "    df = None\n",
    "    is_prequery = (\n",
    "        collector.assistant_sql_code_class.strip() == \"incomplete\"\n",
    "        if collector.assistant_sql_code_class\n",
    "        else False\n",
    "    )\n",
    "\n",
    "    if is_prequery:\n",
    "        try:\n",
    "            print(\"Ejecutando codigo SQL de prequery ...\")\n",
    "            df = run_sql(collector.sql_pre_query)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al ejecutar collector.sql_pre_query \\n{e}\")\n",
    "            traceback.print_exc()\n",
    "        try:\n",
    "            complex_request_sql_summary_response(\n",
    "                llm=langchain_llm,\n",
    "                collector=collector,\n",
    "                llm_collector=llm_collector,\n",
    "                dataframe=df,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error al ejecutar complex_request_sql_summary_response\\n{e}\")\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        try:\n",
    "            print(\"Ejecutando codigo SQL ...\")\n",
    "            df = run_sql(collector.sql_code)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al ejecutar collector.sql_code \\n{e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "        try:\n",
    "            complex_request_sql_summary_response(\n",
    "                llm=langchain_llm,\n",
    "                collector=collector,\n",
    "                llm_collector=llm_collector,\n",
    "                dataframe=df,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error al ejecutar complex_request_sql_summary_response\\n{e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    \n",
    "def post_process_pipeline(\n",
    "    collector: AppDataCollector, llm_collector: LLMResponseCollector\n",
    "):\n",
    "    try:\n",
    "        query_post_process(\n",
    "            llm=langchain_llm,\n",
    "            collector=collector,\n",
    "            llm_collector=llm_collector,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error al ejecutar query_post_process\\n{e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "def generate_sql(\n",
    "    memory: Memory,\n",
    "    collector: AppDataCollector,\n",
    "    llm_collector: LLMResponseCollector,\n",
    "):\n",
    "    pre_process_pipeline(\n",
    "        memory=memory, collector=collector, llm_collector=llm_collector\n",
    "    )\n",
    "    is_simple = collector.request_type.lower().strip() == \"simple\"\n",
    "\n",
    "    if is_simple:\n",
    "        simple_request_pipeline(\n",
    "            collector=collector, llm_collector=llm_collector\n",
    "        )\n",
    "    else:\n",
    "        complex_request_pipeline(\n",
    "            collector=collector, llm_collector=llm_collector, memory=memory\n",
    "        )\n",
    "        post_sql_generation_pipeline(collector, llm_collector)\n",
    "        \n",
    "    post_process_pipeline(\n",
    "        collector=collector, llm_collector=llm_collector\n",
    "    )\n",
    "     \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chat(\n",
    "    user_message: str,\n",
    "    memory: Memory,\n",
    "    collector: AppDataCollector,\n",
    "    llm_collector: LLMResponseCollector,\n",
    "):\n",
    "    last_user_message = memory.add_user_message(user_message)\n",
    "    collector.current_conversation_data.last_user_message = last_user_message\n",
    "    \n",
    "    generate_sql(\n",
    "        memory=memory,\n",
    "        collector=collector,\n",
    "        llm_collector=llm_collector,\n",
    "    )\n",
    "    \n",
    "    ai_message = collector.ai_post_response\n",
    "    dataframe : pd.DataFrame | None = collector.dataframe_response\n",
    "    \n",
    "    last_ai_message = memory.add_ai_message(ai_message, last_user_message.message_id, dataframe)\n",
    "    collector.current_conversation_data.last_ai_message = last_ai_message\n",
    "    \n",
    "    return collector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate-request\n",
      "{'text': '\\nsummary:  The conversation starts with the user asking the assistant to provide the maximum static pressure from the second option in the list.\\nuser_intent: The user is asking for the maximum static pressure from the second option in the list.\\nslots: None'} \n",
      "\n",
      "\n",
      "enhanced-request\n",
      "{'text': '\\nresponse:  The user is asking for the maximum static pressure from a specific option.'} \n",
      "\n",
      "\n",
      "request-type\n",
      "{'text': '\\ntype:  complex\\nanalysis: This input is related to getting information from a specific option, which is likely related to a measurement system database, but I do not have access to it.'} \n",
      "\n",
      "\n",
      "complex_request_process_modification\n",
      "technical-terms\n",
      "{'text': '\\nterms: static pressure, second option, list'} \n",
      "\n",
      "\n",
      "enhanced-request\n",
      "{'text': '\\nresponse:  The user is asking for the maximum static pressure from a specific option.'} \n",
      "\n",
      "\n",
      "has-multi-definition\n",
      "{'text': '\\nclass: clear\\nanalysis: The term \"static pressure\" has a single definition, which is a variable read in the measurement system, as per the provided definition. The sentence context clearly refers to this specific definition, making it clear what the user is asking for.'} \n",
      "\n",
      "\n",
      "flavored-request\n",
      "{'text': '\\nmodified_sentence: The user is asking for the maximum variable from a specific option.'} \n",
      "\n",
      "\n",
      "complex_request_process_semantics\n",
      "semantic-tables\n",
      "{'text': '\\ntables:  med_sistema_medicion, var_tipo_variable, var_variable_datos, med_tag, teq_tipo_equipo'} \n",
      "\n",
      "\n",
      "complex_request_sql_generation\n",
      "generate-sql\n",
      "{'text': \"\\nsql_query:  SELECT MAX(v.Valor) AS 'Max Static Pressure' FROM dbo_v2.var_variable_datos AS v INNER JOIN dbo_v2.med_sistema_medicion AS m ON v.idSistemaMedicion_fk = m.Id INNER JOIN dbo_v2.var_tipo_variable AS vt ON v.IdVariable_fk = vt.Id WHERE m.Tag = 'EMED-3138.12-050' AND vt.Nombre = 'Pressão Estática (kPa)' AND v.Fecha BETWEEN '2023-03-01' AND '2023-03-31'\\nsuggestion: The query is correct, but it would be a good idea to include a check for null values in the WHERE clause to avoid any potential errors.\\nused_tables: var_variable_datos, med_sistema_medicion, var_tipo_variable\"} \n",
      "\n",
      "\n",
      "complex_request_sql_verification\n",
      "classify-sql\n",
      "{'text': \"\\nclass: complete\\n\\nanalysis: This SQL query is well-formed and can be executed as it is.\\n\\nsuggestion: The query is complete and does not require any additional information to execute. However, it's recommended to consider adding a filter for the Estado column in the med_sistema_medicion table to ensure that only active measurement systems are considered.\\n\\nsuggested_sql: None\"} \n",
      "\n",
      "\n",
      "Ejecutando codigo SQL ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lauth\\OneDrive\\Desktop\\sql_assistant_v3\\src\\utils\\sql_utils.py:8: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(sql, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary-response-sql\n",
      "{'text': 'response: The maximum static pressure for the specific option is 568.865.'} \n",
      "\n",
      "\n",
      "post-process-translation\n",
      "{'text': 'detected_language: English\\nresponse: The maximum static pressure for the second option in the list is 568.865.'} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejecutando\n",
    "\n",
    "# Iniciando collector\n",
    "collector = AppDataCollector()\n",
    "llm_collector = LLMResponseCollector()\n",
    "# memory.clear_memory()\n",
    "\n",
    "res = chat(\n",
    "    \"now give me the max static pressure from the second one in list\",\n",
    "    memory,\n",
    "    collector,\n",
    "    llm_collector,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "Your are a very helpfull assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "I need your help with a very important task for my work. Follow carefully this instructions step by step.\n",
      "\n",
      "First, read this current user intent:\n",
      "The next is a conversation between Assistant and Human.\n",
      "<user_intent>The user is asking for the maximum static pressure from the second option in the list.</user_intent> \n",
      "\n",
      "Second, the next are relevant slots from a conversation that are necessary to complete the previous user intent:\n",
      "<Slots>None</Slots>\n",
      "\n",
      "Third, complementation. Add to the user intent the necessary slots to generate a complete and better user intent.\n",
      "\n",
      "Finally, evaluation:\n",
      "- The user intent should contain the relevant slots. A correct intent should answer the question: \"What does the user want?\". \n",
      "- Do not add your own ideas or clarifications or recommendations.\n",
      "\n",
      "Output format response:\n",
      "The output should be formatted with the key format below. Do not add anything beyond the key format.\n",
      "Start Key format:\n",
      "\"response\" is the key and its content: Detailed current user goal, it may starts with \"The user is ...\".\n",
      "End of Key format\n",
      "\n",
      "Begin!<|eot_id|><|start_header_id|>\n",
      "assistant<|end_header_id|>\n",
      "response: \n"
     ]
    }
   ],
   "source": [
    "print(len(llm_collector.llm_responses))\n",
    "print(llm_collector.llm_responses[1].prompt)\n",
    "# Hay que agregar a la lista el nuevo mensaje generado por el asistente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(collector.current_conversation_data.current_slots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyperclip\n",
    "print(len(llm_collector.llm_responses))\n",
    "print(llm_collector.llm_responses[0].prompt)\n",
    "\n",
    "pyperclip.copy(llm_collector.llm_responses[8].prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(collector.terms_dictionary, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<Human>: 2024-07-20 20:57:04.537012\n",
      "Hola!\n",
      "<Human>: 2024-07-20 20:57:27.085011\n",
      "Hola!\n",
      "<Human>: 2024-07-20 20:58:17.618424\n",
      "Hola!\n",
      "<Human>: 2024-07-20 20:58:46.889084\n",
      "Hola!\n",
      "<Assistant>: 2024-07-20 20:58:52.493341\n",
      "¡Hola! Me alegra poder ayudarte. ¿En qué puedo asistirte hoy?\n",
      "<Human>: 2024-07-20 21:00:13.559027\n",
      "me puedes dar una lista de sistemas de medicion\n",
      "<Assistant>: 2024-07-20 21:01:31.993822\n",
      "Aquí está una lista de sistemas de medición: EMED-3138.11-128, EMED-3138.12-050, EMED-3138.12-052, EMED-3138.12-065, EMED-3138.12-105, EST-3138.00-CONS_UDM, EST-3138.12-PILOTO, EST-LIBTQ-CX, FQIT-3138.12-015, TANQ-3138.11-011.\n",
      "Here is a dataframe from SQL: \n",
      "|    | Measurement_System_Name   | Measurement_System_Tag   |\n",
      "|---:|:--------------------------|:-------------------------|\n",
      "|  0 | EMED-3138.11-128          | EMED-3138.11-128         |\n",
      "|  1 | EMED-3138.12-050          | EMED-3138.12-050         |\n",
      "|  2 | EMED-3138.12-052          | EMED-3138.12-052         |\n",
      "|  3 | EMED-3138.12-065          | EMED-3138.12-065         |\n",
      "|  4 | EMED-3138.12-105          | EMED-3138.12-105         |\n",
      "|  5 | EST-3138.00-CONS_UDM      | EST-3138.00-CONS_UDM     |\n",
      "|  6 | EST-3138.12-PILOTO        | EST-3138.12-PILOTO       |\n",
      "|  7 | EST-LIBTQ-CX              | EST-LIBTQ-CX             |\n",
      "|  8 | FQIT-3138.12-015          | FQIT-3138.12-015         |\n",
      "|  9 | TANQ-3138.11-011          | TANQ-3138.11-011         |\n"
     ]
    }
   ],
   "source": [
    "print(memory.list_chat_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlglot\n",
    "import pyperclip\n",
    "\n",
    "sql = collector.sql_code\n",
    "code = sqlglot.transpile(sql, write=\"tsql\", identify=True, pretty=True)[0]\n",
    "pyperclip.copy(code)\n",
    "print(code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
