{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:\\\\Users\\\\lauth\\\\OneDrive\\\\Desktop\\\\sql_assistant_v3\")\n",
    "\n",
    "from src.components.collector.collector import AppDataCollector, LLMResponseCollector\n",
    "from src.components.models.llms.llms import Langchain_OpenAI_LLM, HF_Llama38b_LLM\n",
    "from src.components.models.embeddings.embeddings import HF_MultilingualE5_Embeddings, Openai_Embeddings\n",
    "from src.components.memory.memory import Memory\n",
    "\n",
    "langchain_llm = Langchain_OpenAI_LLM()\n",
    "langchain_llm.init_model()\n",
    "\n",
    "hf_llm = HF_Llama38b_LLM()\n",
    "hf_llm.init_model()\n",
    "\n",
    "mle5_embeddings = HF_MultilingualE5_Embeddings()\n",
    "mle5_embeddings.init_model()\n",
    "\n",
    "openai_embeddings = Openai_Embeddings()\n",
    "openai_embeddings.init_model()\n",
    "\n",
    "memory = Memory()\n",
    "\n",
    "collector = AppDataCollector()\n",
    "llm_collector = LLMResponseCollector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector.user_request = \"The human is seeking detailed information, specifically the average temperature for a specific location identified as EMED-3138.12-050, in a specific month and year, August 2023.\"\n",
    "collector.request_type = \"complex\"\n",
    "collector.conversation_summary = \"The conversation asked for the average temperature for EMED-3138.12-050 in August 2023.\"\n",
    "collector.terms_dictionary = [\n",
    "    {\n",
    "        \"original_term\": \"average temperature\",\n",
    "        \"cleaned_term\": \"average temperature\",\n",
    "        \"definitions\": [\n",
    "            {\n",
    "                \"sql_instructions\": \"Use 'Temperatura (\\u00b0C)' in the WHERE statement when refers to temperature.\",\n",
    "                \"table_name\": \"var_tipo_variable\",\n",
    "                \"standard_term\": \"variable\",\n",
    "                \"definition\": \"When talking about the term 'temperature' or similar, this refers to a variable read in the measurement system.\",\n",
    "                \"replace_instruction\": \"The term 'temperature' or related refers to a variable read in measurement system, replace with 'variable'.\",\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"original_term\": \"EMED-3138.12-050\",\n",
    "        \"cleaned_term\": \"emed\",\n",
    "        \"definitions\": [\n",
    "            {\n",
    "                \"sql_instructions\": \"\",\n",
    "                \"table_name\": \"med_sistema_medicion\",\n",
    "                \"standard_term\": \"measurement system\",\n",
    "                \"definition\": \"When talking about the term 'EMED' or similar, it refers to the name/tag of the measurement system.\",\n",
    "                \"replace_instruction\": \"When talking about 'EMED' as a measurement system replace with 'measurement system' instead.\",\n",
    "            },\n",
    "            {\n",
    "                \"sql_instructions\": \"When context is related to flow computer firmware, 'EMED-010' could be the name of the flow computer firmware.\",\n",
    "                \"table_name\": \"fcs_firmware\",\n",
    "                \"standard_term\": \"firmware\",\n",
    "                \"definition\": \"Terms related to 'EMED-010' sometimes are names of flow computers firmwares/type\",\n",
    "                \"replace_instruction\": \"When talking about 'EMED-010' as a flow computer firmware, replace with 'firmware' instead of EMED-010.\",\n",
    "            },\n",
    "            {\n",
    "                \"sql_instructions\": \"\",\n",
    "                \"table_name\": \"fcs_computadores\",\n",
    "                \"standard_term\": \"flow computer\",\n",
    "                \"definition\": \"When talking about the term 'EMED' or similar, it refers to the name of flow computer.\",\n",
    "                \"replace_instruction\": \"When talking about 'EMED' as a flow computer replace with 'flow computer' instead.\",\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate-request\n",
      "{'text': '\\nintention:  The human is requesting information about a specific platform with associated id 33.'} \n",
      "\n",
      "\n",
      "request-type\n",
      "{'text': \"\\ntype:  complex\\nanalysis: This input is related to get information from the measurement system database, but I don't have access to it, so it is classified as complex.\"} \n",
      "\n",
      "\n",
      "The human is requesting information about a specific platform with associated id 33.\n",
      "complex\n"
     ]
    }
   ],
   "source": [
    "# primer request y clasificacion\n",
    "\n",
    "from src.app.pipeline_processes.query_pre_process.manager import query_pre_process\n",
    "\n",
    "memory.clear_memory()\n",
    "memory.add_user_message(\"Hello, my name is Lauther\")\n",
    "memory.add_ai_message(\"Hi Lauther, I'm M-bot, how can I help you?\")\n",
    "memory.add_user_message(\"Which platform has id 33?\")\n",
    "\n",
    "request = query_pre_process(hf_llm, memory, collector, llm_collector)\n",
    "print(request.user_request)\n",
    "print(request.request_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary-conversation\n",
      "{'text': '\\nsummary:  Lauther initiated a conversation with M-bot, asking which platform has ID 33.'} \n",
      "\n",
      "\n",
      "Lauther initiated a conversation with M-bot, asking which platform has ID 33.\n"
     ]
    }
   ],
   "source": [
    "# Resumiendo la conversacion\n",
    "\n",
    "from src.components.memory import MEMORY_TYPES\n",
    "from src.app.rag.rag_utils import base_llm_generation\n",
    "\n",
    "# Prompting\n",
    "conversation_summary_instructions = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "Your are a very helpfull assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Following the next conversation\n",
    "{chat_history}\n",
    "END OF CONVERSATION\n",
    "\n",
    "Your task is to describe and summarize the conversation.\n",
    "\n",
    "Here are some advices for a better response:\n",
    " - You may add sensitive information to your response, like names or technical terms that are mentioned in conversation.\n",
    " - Do not include any explanations or apologies in your response.\n",
    " - Do not add your own conclusions or clarifications.\n",
    " - Do not add words nor nouns nor adjectives to complement the response if these are not mentioned in the conversation.\n",
    "\n",
    "Output format response:\n",
    "The output should be formatted with the key format below. Do not add anything beyond the key format.\n",
    "Start Key format:\n",
    "\"summary\" is the key and its content is: Brief detailed summary of the conversation . . .\n",
    "End of Key format\"\"\"\n",
    "conversation_summary_suffix = \"summary: \"\n",
    "\n",
    "def get_conversation_summary_prompt(memory: Memory):\n",
    "    current_messages = memory.get_current_messages()\n",
    "    chat_history = \"\"\n",
    "    for message in current_messages:\n",
    "        m = message[\"content\"]\n",
    "        if message[\"type\"] == MEMORY_TYPES[\"AI\"]:\n",
    "            chat_history += f\"AI Message: {m}\\n\"\n",
    "        else:\n",
    "            chat_history += f\"Human Message: {m}\\n\"\n",
    "\n",
    "    prompt = conversation_summary_instructions.format(chat_history=chat_history)\n",
    "    suffix = conversation_summary_suffix\n",
    "    return prompt, suffix\n",
    "\n",
    "instruction, suffix = get_conversation_summary_prompt(memory)\n",
    "\n",
    "# Generation\n",
    "prompt = hf_llm.apply_model_template(instruction, suffix)\n",
    "res = base_llm_generation(hf_llm, llm_collector, prompt, \"summary-conversation\")\n",
    "\n",
    "# Manager\n",
    "collector.conversation_summary = str(res[\"summary\"]).strip()\n",
    "print(collector.conversation_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.app.pipeline_processes.sql_pre_process.generation import generate_technical_terms\n",
    "from src.app.pipeline_processes.sql_pre_process.retrievers import retrieve_semantic_term_definitions, retrieve_terms_examples\n",
    "from src.settings.settings import Settings\n",
    "import json\n",
    "\n",
    "# Extrayendo las palabras tecnicas\n",
    "\n",
    "# Retriever\n",
    "conversation_summary = collector.conversation_summary\n",
    "terms_examples = retrieve_terms_examples(conversation_summary, mle5_embeddings)\n",
    "\n",
    "# Generation\n",
    "output = generate_technical_terms(hf_llm, llm_collector, conversation_summary, terms_examples)\n",
    "technical_terms = output[\"terms\"]\n",
    "\n",
    "# Parte 2: Recuperando definiciones para los terminos desde la bd vectorial (retrieval) para crear el diccionario\n",
    "terms_collection = Settings.Chroma.get_terms_collection()\n",
    "terms_dictionary, has_replacement_definitions, _ = (\n",
    "    retrieve_semantic_term_definitions(\n",
    "        mle5_embeddings, terms_collection, technical_terms\n",
    "    )\n",
    ")\n",
    "\n",
    "collector.terms_dictionary = terms_dictionary\n",
    "print(json.dumps(collector.terms_dictionary, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enhanced-request-conversation\n",
      "{'text': '\\nresponse:  The human is trying to identify the specific platform with the given unique identifier, id 33.'} \n",
      "\n",
      "\n",
      "{'response': ' The human is trying to identify the specific platform with the given unique identifier, id 33.'}\n"
     ]
    }
   ],
   "source": [
    "from src.components.memory import MEMORY_TYPES\n",
    "from src.app.rag.rag_utils import base_llm_generation\n",
    "\n",
    "# Generando un nuevo request con el diccionario\n",
    "request_generation_prompt = \"\"\"I need your help, I'm trying to generate a summarize request from a user in a conversation. Follow carefully the next steps.\n",
    "\n",
    "First, look up the next messages between HUMAN and AI:\n",
    "'''\\n{chat_history}\\n'''\n",
    "\n",
    "Second, look up the next relevant definitions from dictionary:\n",
    "''''\\n{terms}\\n''\n",
    "\n",
    "Third, analyze the messages and briefly describe in one line the human intention and what he is looking for or doing. If you have to complement a term, use only definitions on previous dictionary.\n",
    "\n",
    "Fourth, evaluation. Evaluate if your response has the necessary information retrieved from the conversation and dictionary. Also it must starts with: 'The human is ...' .\n",
    "  \n",
    " - Pay attention if the last message refers to previous ones to add necessary information located in previous messages.\n",
    " - You may add sensitive information to your response, like names or technical terms that are mentioned in conversation.\n",
    " - Do not include any explanations or apologies in your response.\n",
    " - Do not add your own conclusions or clarifications.\n",
    " - Do not add your own thoughts about the request.\n",
    "\n",
    "Output format response:\n",
    "The output should be formatted with the key format below. Do not add anything beyond the key format.\n",
    "Start Key format:\n",
    "\"response\" is the key and its content is: Detailed user request. It may start with The human is . . .\n",
    "End of Key format\n",
    "Begin!\"\"\"\n",
    "\n",
    "request_generation_suffix=\"response: \"\n",
    "\n",
    "\n",
    "def get_request_generation_prompt(memory: Memory, terms_dictionary: dict[str, any]):\n",
    "    current_messages = memory.get_current_messages()\n",
    "    chat_history = \"\"\n",
    "    for message in current_messages:\n",
    "        m = message[\"content\"]\n",
    "        if message[\"type\"] == MEMORY_TYPES[\"AI\"]:\n",
    "            chat_history += f\"AI Message: {m}\\n\"\n",
    "        else:\n",
    "            chat_history += f\"Human Message: {m}\\n\"\n",
    "            \n",
    "    definitions=[]\n",
    "        \n",
    "    if terms_dictionary:\n",
    "        for item in terms_dictionary:\n",
    "            for inner_item in item[\"definitions\"]:\n",
    "                definitions.append(str(inner_item[\"definition\"]).strip())\n",
    "        terms = \"    - \"\n",
    "        \n",
    "        content = \"\\n    - \".join(definitions)\n",
    "        terms += f\"{content}\\n\"\n",
    "\n",
    "    instructions = request_generation_prompt.format(chat_history=chat_history, terms=terms)\n",
    "    suffix = request_generation_suffix\n",
    "    return instructions, suffix\n",
    "\n",
    "\n",
    "# Generation\n",
    "instruction, suffix = get_request_generation_prompt(memory, collector.terms_dictionary)\n",
    "prompt  = hf_llm.apply_model_template(instruction, suffix)\n",
    "res = base_llm_generation(hf_llm, llm_collector, prompt, \"enhanced-request-conversation\")\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
